{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3aii: Random Isotropic Perturbations at Thought Anchors (KL vs. ε)\n",
    "\n",
    "This notebook mirrors Experiment 3a's setup but replaces targeted steering with random isotropic perturbations in the residual stream at selected layers.\n",
    "\n",
    "Protocol:\n",
    "- Use the same anchor/counterfactual prompt pairs as in 3a (from the annotated dataset).\n",
    "- At the token just before the anchor, sample random unit directions in hidden space.\n",
    "- Scale each perturbation by ε times the RMS of the (unperturbed) residual at that token/layer.\n",
    "- Compute KL divergence over the anchor chunk next-token distributions vs. the unperturbed run.\n",
    "- Plot KL vs. ε, averaging over random directions (and examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, random, gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, pathlib\n",
    "repo_root = pathlib.Path(__file__).resolve().parents[1]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "from utils import load_model_and_vectors, split_solution_into_chunks, get_char_to_token_map\n",
    "\n",
    "# Parameters (keep consistent with 3a where possible)\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "max_examples = 8  # subset for a quick run; increase as needed\n",
    "selected_layers = None  # e.g., [24, 26, 28]; None = use last layer only\n",
    "epsilons = [0.0, 0.25, 0.5, 1.0, 2.0]  # multiples of RMS\n",
    "n_directions = 8  # random directions per epsilon\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "# Resolve device in the same way as other experiments\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer (no features needed)\n",
    "model, tokenizer, _ = load_model_and_vectors(model_name=model_name, compute_features=False, device=device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "hidden_size = model.config.hidden_size\n",
    "num_layers = model.config.num_hidden_layers\n",
    "if selected_layers is None:\n",
    "    selected_layers = [num_layers - 1]  # last layer default\n",
    "selected_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotated dataset produced by Experiment 2\n",
    "model_tag = model_name.replace('/', '-')\n",
    "annotated_path = repo_root / 'generated_data' / f'generated_data_annotated_{model_tag}.json'\n",
    "assert annotated_path.exists(), f'Missing annotated dataset: {annotated_path}'\n",
    "with open(annotated_path, 'r') as f:\n",
    "    annotated = json.load(f)\n",
    "# Filter to examples with an anchor sentence\n",
    "examples = [ex for ex in annotated if ex.get('thought_anchor_sentence')]\n",
    "if max_examples is not None:\n",
    "    examples = examples[:max_examples]\n",
    "len(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anchor_token_span(full_text: str, cot_text: str, anchor_sentence: str):\n",
    "    \"\"\"Locate token-span [start, end) of the anchor sentence within full_text.\n",
    "    First try char-to-token mapping; fallback to sub-token sequence search.\n",
    "    Returns (start_idx, end_idx, think_start_idx_char).\n",
    "    \"\"\"\n",
    "    think_start = full_text.find('<think>')\n",
    "    if think_start < 0:\n",
    "        think_start = 0\n",
    "    # Char mapping approach\n",
    "    try:\n",
    "        offsets = tokenizer.encode_plus(full_text, return_offsets_mapping=True)['offset_mapping']\n",
    "    except Exception:\n",
    "        offsets = None\n",
    "    if offsets:\n",
    "        anchor_char = cot_text.find(anchor_sentence)\n",
    "        if anchor_char >= 0:\n",
    "            # Map cot offset to full_text by adding think_start and '<think>\n' length if applicable\n",
    "            # We assume cot_text appears after '<think>' in full_text\n",
    "            # Find the first occurrence of cot_text in full_text to establish base\n",
    "            cot_in_full = full_text.find(cot_text[:max(1, min(20, len(cot_text)))])\n",
    "            base_char = cot_in_full if cot_in_full >= 0 else think_start\n",
    "            start_char = base_char + anchor_char\n",
    "            end_char = start_char + len(anchor_sentence) - 1\n",
    "            # Build char->token map\n",
    "            char_to_tok = {}\n",
    "            for ti, (s,e) in enumerate(offsets):\n",
    "                for cp in range(s, e):\n",
    "                    char_to_tok[cp] = ti\n",
    "            ts = char_to_tok.get(start_char, None)\n",
    "            te = char_to_tok.get(end_char, None)\n",
    "            if ts is not None and te is not None and ts < te:\n",
    "                return ts, te+1, think_start\n",
    "    # Fallback: sub-token match\n",
    "    ids_full = tokenizer(full_text, return_tensors='pt', add_special_tokens=False)['input_ids'][0].tolist()\n",
    "    ids_anchor = tokenizer(anchor_sentence, return_tensors='pt', add_special_tokens=False)['input_ids'][0].tolist()\n",
    "    # Search after think_start region by limiting to suffix tokens\n",
    "    # Convert char think_start to token index by approximate scan\n",
    "    approx_ts = 0\n",
    "    try:\n",
    "        prefix = tokenizer(full_text[:think_start], return_tensors='pt', add_special_tokens=False)['input_ids'][0]\n",
    "        approx_ts = int(prefix.shape[-1])\n",
    "    except Exception:\n",
    "        approx_ts = 0\n",
    "    hay = ids_full[approx_ts:]\n",
    "    def find_subseq(h, n):\n",
    "        L, M = len(h), len(n)\n",
    "        for i in range(max(0, L-M+1)):\n",
    "            if h[i:i+M] == n:\n",
    "                return i\n",
    "        return -1\n",
    "    pos = find_subseq(hay, ids_anchor)\n",
    "    if pos >= 0:\n",
    "        s = approx_ts + pos\n",
    "        return s, s+len(ids_anchor), think_start\n",
    "    return None, None, think_start\n",
    "\n",
    "def build_full_text(ex):\n",
    "    # Use the exact prompt + generated text saved upstream for consistency\n",
    "    base = ex.get('raw_response') or ''\n",
    "    # If raw_response is missing, reconstruct minimal input: prompt + think + cot\n",
    "    if not base:\n",
    "        prompt = ex.get('model_input_prompt') or ''\n",
    "        cot = ex.get('cot') or ''\n",
    "        base = prompt + cot\n",
    "    return base\n",
    "\n",
    "def kl_between_logits(logits_p, logits_q):\n",
    "    # logits_*: [T, V]\n",
    "    logp = F.log_softmax(logits_p, dim=-1)\n",
    "    logq = F.log_softmax(logits_q, dim=-1)\n",
    "    p = logp.exp()\n",
    "    kl = (p * (logp - logq)).sum(dim=-1)  # [T]\n",
    "    return float(kl.mean().item())\n",
    "\n",
    "def unit_random_direction(dim, device, dtype):\n",
    "    v = torch.randn(dim, device=device, dtype=torch.float32)\n",
    "    v = v / (v.norm() + 1e-12)\n",
    "    return v.to(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {layer: {float(eps): [] for eps in epsilons} for layer in selected_layers}\n",
    "\n",
    "for idx, ex in enumerate(examples, start=1):\n",
    "    question_preview = (ex.get('prompt','') or '').replace('\n',' ')[:80]\n",
    "    anchor_sentence = ex.get('thought_anchor_sentence') or ''\n",
    "    cot_text = ex.get('cot') or ''\n",
    "    full_text = build_full_text(ex)\n",
    "\n",
    "    # Encode up to at least end of anchor sentence to compute logits teacher-forced\n",
    "    tok = tokenizer(full_text, return_tensors='pt', add_special_tokens=False)\n",
    "    ids_full = tok['input_ids'].to(model.device)\n",
    "    attn_full = (ids_full != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "    # Locate anchor token span and choose the token just before anchor\n",
    "    ts, te, think_char = find_anchor_token_span(full_text, cot_text, anchor_sentence)\n",
    "    if ts is None or te is None or ts <= 0:\n",
    "        print(f'[skip] could not locate anchor span for example {idx}: {question_preview}')\n",
    "        continue\n",
    "    token_before = ts - 1\n",
    "\n",
    "    # Baseline trace: capture logits and per-layer RMS at token_before\n",
    "    with model.trace({\n",
    "        'input_ids': ids_full,\n",
    "        'attention_mask': attn_full,\n",
    "    }) as tracer:\n",
    "        # Save layer outputs for RMS and final logits\n",
    "        resid_saves = []\n",
    "        for L in selected_layers:\n",
    "            resid_saves.append(model.model.layers[L].output[0].save())\n",
    "        logits_base = model.lm_head.output.save()\n",
    "    \n",
    "    # Extract tensors\n",
    "    logits_base = logits_base[0, ts:te, :].detach().to(torch.float32).cpu()  # [T,V]\n",
    "    rms_by_layer = {}\n",
    "    for L, rs in zip(selected_layers, resid_saves):\n",
    "        h = rs[0, token_before, :].detach().to(torch.float32)\n",
    "        rms = float((h.pow(2).mean().sqrt().item()))\n",
    "        rms_by_layer[L] = rms if math.isfinite(rms) and rms > 0 else 1e-3\n",
    "    del resid_saves\n",
    "    gc.collect()\n",
    "\n",
    "    # For each layer, run perturbations for each epsilon and average KL over directions\n",
    "    for L in selected_layers:\n",
    "        rms = rms_by_layer[L]\n",
    "        dtype = torch.bfloat16 if hasattr(model, 'dtype') and str(model.dtype) == 'torch.bfloat16' else torch.bfloat16\n",
    "        for eps in epsilons:\n",
    "            kls = []\n",
    "            if eps == 0.0:\n",
    "                # KL(base || base) = 0 for reference\n",
    "                results[L][float(eps)].append(0.0)\n",
    "                continue\n",
    "            for _ in range(n_directions):\n",
    "                v = unit_random_direction(hidden_size, model.device, dtype)\n",
    "                delta = (eps * rms) * v\n",
    "                with model.trace({\n",
    "                    'input_ids': ids_full,\n",
    "                    'attention_mask': attn_full,\n",
    "                }) as tracer:\n",
    "                    # Inject at the single position token_before\n",
    "                    model.model.layers[L].output[0][0, token_before, :] += delta\n",
    "                    logits_p = model.lm_head.output.save()\n",
    "                logits_p = logits_p[0, ts:te, :].detach().to(torch.float32).cpu()\n",
    "                kl = kl_between_logits(logits_p, logits_base)\n",
    "                kls.append(kl)\n",
    "            if kls:\n",
    "                results[L][float(eps)].append(float(sum(kls)/len(kls)))\n",
    "\n",
    "# Aggregate per-layer averages across examples\n",
    "summary = {L: {eps: (sum(vals)/len(vals) if vals else 0.0) for eps, vals in eps_map.items()} for L, eps_map in results.items()}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KL vs. epsilon per selected layer\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "for L in selected_layers:\n",
    "    xs = [float(e) for e in epsilons]\n",
    "    ys = [summary[L].get(float(e), 0.0) for e in epsilons]\n",
    "    ax.plot(xs, ys, marker='o', label=f'layer {L}')\n",
    "ax.set_xlabel('epsilon (× RMS at token_before)')\n",
    "ax.set_ylabel('KL(perturbed || base) over anchor chunk')\n",
    "ax.set_title('Random isotropic residual perturbations at anchor (KL vs. ε)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

